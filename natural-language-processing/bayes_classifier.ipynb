{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zajęcia 7: Bayes Classifier\n",
    "\n",
    "Wszystkie zadania ćwiczeniowe należy rozwiązywać w języku Python w kopii Jupyter Notebook'a dla danych zajęć w wyznaczonych miejscach (komórki z komentarzem `# Solution`).\n",
    "\n",
    "Nie należy usuwać komórek z treścią zadań.\n",
    "\n",
    "Należy wyświetlać outputy przy pomocy `print`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Czym jest Bag of Words (BoW)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Na czym polega \"naiwność\" klasyfikatora Bayesa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kroki do zaimplementowania klasyfikatora Bayesa:\n",
    "![alt text](bayes_steps.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(C|X) = P(C) * P(X|C)\n",
    "\n",
    "P(C)\n",
    "\n",
    "15 - tekstów\n",
    "<br>\n",
    "5 - spam\n",
    "<br>\n",
    "10 - niespam\n",
    "\n",
    "P(spam) = 5/15 = 1/3\n",
    "\n",
    "<br>\n",
    "P(X|C)\n",
    "\n",
    "Lubię borowiki i kurki.                                 NIE SPAM\n",
    "<br>\n",
    "Lubię borowiki, borowiki i jeszcze raz borowiki.        NIE SPAM\n",
    "<br>\n",
    "Poszedłem na zakupy.                                    NIE SPAM\n",
    "\n",
    "P(borowiki|NIE SPAM) = 4 / 14\n",
    "\n",
    "P(zakupy|NIE SPAM) = 1 / 14\n",
    "\n",
    "P(X|NIE SPAM) = 4/14 * 1/14 * ...\n",
    "\n",
    "\n",
    "\n",
    "100 tekstów ze spamem, każde ma po 8 słów (i w pozostałych 99 nie występuje słowo borowiki)\n",
    "<br>\n",
    "1                                                       SPAM\n",
    "<br>\n",
    "2                                                       SPAM\n",
    "<br>\n",
    "...\n",
    "<br>\n",
    "Kup kup nasze nasze borowiki borowiki (TANIO TANIO)!!!  SPAM\n",
    "<br>\n",
    "...\n",
    "<br>\n",
    "99                                                      SPAM\n",
    "<br>\n",
    "100                                                     SPAM\n",
    "\n",
    "P(borowiki|SPAM) = 2 / 800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferencja (czyli dokonanie predykcji modelu) na zbiorze testowym\n",
    "\n",
    "Dla zdania ze zbioru testowego dla każdego słowa, które znajduje się w słowniku, obliczamy wartości P(xi|C), gdzie wartość z licznika oraz mianiownika bazuje na danych ze zbioru treningowego.\n",
    "\n",
    "Czyli np. dla zdania \"Kup pyszne borowiki\" jeżeli tylko słowa \"kup\" oraz \"pyszne\" występują w słowniku obliczamy P(X|C) = P(kup|C) * P(borowiki|C).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zadanie 1\n",
    "\n",
    "Zaimplementuj i wytrenuj klasyfikator Bayesa (bez korzystania z gotowych implementacji algorytmu) na danych treningowych z wyzwania:\n",
    "\n",
    "https://amueval.pl/challenge/Spamcl4Ssificationsms/\n",
    "\n",
    "A następnie wygeneruj predykcje dla zbioru testowego i dokonaj zgłoszenia na stronie wyzwania w zakładce \"Add Submission\".\n",
    "\n",
    "W pliku out.tsv muszą znajdować się wartości 0 lub 1 oddzielone nowymi liniami (bez nagłówka).\n",
    "\n",
    "Można spróbować z różnymi wartościami alpha oraz liczbą słów w słowniku (czyli w kroku P(X|C) możemy uwzględniać tylko np. 500 najczęściej występujących słów - nie gwarantuję że to pomoże).\n",
    "\n",
    "Proszę pamiętać o redukcji obliczeń (np. liczbę słów w danej klasie można zapisać w zmiennej, a nie robić to za każdym razem).\n",
    "\n",
    "#### Proszę podać nr indeksu przy wysyłaniu zadania na Teamsach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "# NR INDEKSU: 481825\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# used for laplace smoothing\n",
    "alpha=0.32\n",
    "classes=[0,1]\n",
    "\n",
    "def read_and_prepare_data(messages_file, labels_file):\n",
    "    messages=pd.read_csv(messages_file, sep='\\t', header=None)\n",
    "    labels=pd.read_csv(labels_file, sep='\\t', header=None)\n",
    "    # combine messages and labels into one dataframe\n",
    "    alldata=pd.concat([messages, labels], axis=1)\n",
    "    alldata.columns=['message', 'label']\n",
    "    # make all messages lowercase and remove punctuation\n",
    "    alldata['message']=alldata['message'].str.lower()\n",
    "    alldata['message']=alldata['message'].apply(lambda x: re.sub(r'[^\\w\\s]','',x))\n",
    "    return alldata\n",
    "    \n",
    "def get_number_of_documents(data):\n",
    "    return data.shape[0]\n",
    "\n",
    "def get_number_of_documents_in_class(data, class_value):\n",
    "    return data[data['label']==class_value].shape[0]\n",
    "\n",
    "def calculate_prior_prob(data, class_value):\n",
    "    return get_number_of_documents_in_class(data, class_value)/get_number_of_documents(data)\n",
    "\n",
    "def create_vocabulary(data):\n",
    "    vocabulary=set()\n",
    "    for message in data['message']:\n",
    "        words=message.split()\n",
    "        for word in words:\n",
    "            vocabulary.add(word)\n",
    "    return vocabulary\n",
    "\n",
    "def get_total_word_count_in_class(data, class_value):\n",
    "    total_word_count=0\n",
    "    for message in data[data['label']==class_value]['message']:\n",
    "        words=message.split()\n",
    "        total_word_count+=len(words)\n",
    "    return total_word_count\n",
    "\n",
    "# This function calculates the likelihood for all words in the given vocabulary for given class\n",
    "def calculate_likelihood_for_vocab(data,vocab,class_value):\n",
    "    likelihood={}\n",
    "    total_word_count=total_word_count_for_classes[class_value]\n",
    "    for word in vocab:\n",
    "        word_apperances_count=0\n",
    "        for message in data[data['label']==class_value]['message']:\n",
    "            words=message.split()\n",
    "            word_apperances_count+=words.count(word)\n",
    "        likelihood[word]=(word_apperances_count+alpha)/(total_word_count+len_vocab)\n",
    "    return likelihood\n",
    "\n",
    "def probablity_of_sentence_for_class(sentence,class_value):\n",
    "    likelihood = likelihood_not_spam if class_value == 0 else likelihood_spam\n",
    "    # 1 is neutral element for multiplication\n",
    "    prob=1\n",
    "    words=sentence.split()\n",
    "    for word in words:\n",
    "        if(word in likelihood):\n",
    "            prob*=likelihood[word]\n",
    "        else:\n",
    "            # if word is not in the vocabulary, we calculate probability as before but without word apperances in that class (it's seen for the first time)\n",
    "            prob*=(alpha/(total_word_count_for_classes[class_value]+(alpha*len_vocab)))\n",
    "    return prob\n",
    "\n",
    "def calculate_posterior_prob_for_class(sentence,class_value):\n",
    "    prior=prior_prob_for_classes[class_value]\n",
    "    prob_for_words=probablity_of_sentence_for_class(sentence, class_value)\n",
    "    return prior*prob_for_words\n",
    "\n",
    "def classify(sentence):\n",
    "    sentence=sentence.lower()\n",
    "    sentence=re.sub(r'[^\\w\\s]','',sentence)\n",
    "    prob_not_spam=calculate_posterior_prob_for_class(sentence, 0)\n",
    "    prob_spam=calculate_posterior_prob_for_class(sentence, 1)\n",
    "    # if both probabilities are equal, it is better to classify as not spam (we don't want to classify something more likely to non spam as spam)\n",
    "    if(prob_not_spam==prob_spam):\n",
    "        return 0\n",
    "    elif(prob_not_spam>prob_spam):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "data=read_and_prepare_data('spam-classification/train/in.tsv', 'spam-classification/train/expected.tsv')\n",
    "vocab=create_vocabulary(data)\n",
    "len_vocab=len(vocab)\n",
    "\n",
    "# calculate total word count for both classes and store results in a dictionary\n",
    "total_word_count_for_classes={\n",
    "        0: get_total_word_count_in_class(data, 0),\n",
    "        1: get_total_word_count_in_class(data, 1)\n",
    "    }\n",
    "\n",
    "# calculate prior probabilities for both classes and store them in a dictionary\n",
    "prior_prob_for_classes={\n",
    "        0: calculate_prior_prob(data, 0),\n",
    "        1: calculate_prior_prob(data, 1)\n",
    "    }\n",
    "\n",
    "# calculate likelihood for all words in the vocabulary for both classes and store them in dictionaries\n",
    "likelihood_not_spam=calculate_likelihood_for_vocab(data, vocab, 0)\n",
    "likelihood_spam=calculate_likelihood_for_vocab(data, vocab, 1)\n",
    "\n",
    "\n",
    "test_data=pd.read_csv('spam-classification/test/in.tsv', sep='\\t', header=None)\n",
    "result=pd.DataFrame()\n",
    "\n",
    "for sentence in test_data[0]:\n",
    "    result = pd.concat([result, pd.DataFrame([[classify(sentence)]] )], ignore_index=True)\n",
    "    \n",
    "result.to_csv('spam-classification/test/out.tsv', sep='\\t', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
