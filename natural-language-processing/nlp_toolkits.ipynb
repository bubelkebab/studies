{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zajęcia 6: NLP Toolkits\n",
    "\n",
    "Wszystkie zadania ćwiczeniowe należy rozwiązywać w języku Python w kopii Jupyter Notebook'a dla danych zajęć w wyznaczonych miejscach (komórki z komentarzem `# Solution`).\n",
    "\n",
    "Nie należy usuwać komórek z treścią zadań.\n",
    "\n",
    "Należy wyświetlać outputy przy pomocy `print`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dla chętnych - może się przydać do projektu końcowego\n",
    "Gorąco zachęcam do sprawdzenia zasosób dla języka polskiego: https://github.com/sdadas/polish-nlp-resources "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UWAGA\n",
    "Każde zadanie należy przetestować na własnych tekstach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 1 \n",
    "\n",
    "Przy pomocy toolkitu NLTK napisz funkcję, która na wejściu przyjmuje tekst w języku angielskim, dokonuje tokenizacji na poziomie słów oraz zdań i wypisuje następujące informacje:\n",
    "* liczba znaków w tekście\n",
    "* liczba słów (tokenów według tokenizera NLTK) w tekście\n",
    "* liczba zdań (według sentence tokenizera NLTK) w tekście"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in text: 379\n",
      "Number of words in text: 67\n",
      "Number of sentences in text: 2\n"
     ]
    }
   ],
   "source": [
    "# Solution 1\n",
    "\n",
    "import nltk\n",
    "\n",
    "def tokenize_and_process_text(text):\n",
    "    characters_count=len(text)\n",
    "    \n",
    "    tokens=nltk.word_tokenize(text)\n",
    "    tokens_count=len(tokens)\n",
    "    \n",
    "    sentences=nltk.sent_tokenize(text)\n",
    "    sentences_count=len(sentences)\n",
    "    \n",
    "    print(f\"Number of characters in text: {characters_count}\")\n",
    "    print(f\"Number of words in text: {tokens_count}\")\n",
    "    print(f\"Number of sentences in text: {sentences_count}\")\n",
    "\n",
    "# Source of text: https://lingua.com/english/reading/valentines-day/\n",
    "\n",
    "example=\"\"\"Chocolates and flowers are commonly given as gifts during Valentine's Day, \n",
    "as are accompanying greeting cards (greeting card companies release new Valentine's Day designs annually).\n",
    "Red and pink are generally understood to be \"the colors\" of Valentine's Day, and many individuals, \n",
    "instead of celebrating romantically, spend the holiday with their friends and/or family members.\"\"\"\n",
    "\n",
    "tokenize_and_process_text(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 2 \n",
    "\n",
    "Przy pomocy toolkitu SpaCy napisz funkcję, która na wejściu przyjmuje tekst w języku angielskim oraz jeden z trzech argumentów:\n",
    "* rzeczownik\n",
    "* czasownik\n",
    "* przymiotnik\n",
    "\n",
    "a następnie tokenizuje tekst na słowa i zwraca słowa spełniajace kryterium części mowy określone poprzez drugi argument (czyli np dla argumentu rzeczownik zwraca listę rzeczowników). Należy skorzystać z modułu PoS (part of speech)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of nouns in this text:\n",
      " ['air', 'quality', 'index', 'level', 'range', 'winter', 'smog', 'factors', 'temperatures', 'crop', 'burning', 'dust', 'vehicle', 'emissions', 'wind', 'speeds', 'Residents', 'eyes', 'throats', 'difficulty', 'breathing', 'air', 'purifiers', 'indoors', 'conditions', 'Pollution', 'groups', 'children', 'health', 'problems', 'issues', 'Experts', 'lack', 'term', 'solutions', 'government', 'measures']\n",
      "List of verbs in this text:\n",
      " ['reached', 'struggles', 'caused', 'including', 'experience', 'burning', 'use', 'work', 'facing', 'impacts', 'leading', 'criticize', 'noting']\n",
      "List of adjectives in this text:\n",
      " ['severe', 'ideal', 'various', 'cold', 'low', 'itchy', 'many', 'hazardous', 'vulnerable', 'elderly', 'such', 'respiratory', 'long', 'reactive']\n"
     ]
    }
   ],
   "source": [
    "# Solution 2\n",
    "\n",
    "import spacy\n",
    "\n",
    "def process_part_of_speech(partOfSpeech):\n",
    "    match partOfSpeech:\n",
    "        case \"noun\":\n",
    "            return \"NOUN\"\n",
    "        case \"verb\":\n",
    "            return \"VERB\"\n",
    "        case \"adjective\":\n",
    "            return \"ADJ\"\n",
    "\n",
    "def return_part_of_speech_from_text(text,partOfSpeech):\n",
    "    sp=spacy.load(\"en_core_web_sm\")\n",
    "    doc=sp(text)\n",
    "    pos=process_part_of_speech(partOfSpeech)\n",
    "    found_words=[]\n",
    "    for token in doc:\n",
    "        if token.pos_==pos:\n",
    "            found_words.append(token.text)\n",
    "    print(f\"List of {partOfSpeech+'s'} in this text:\\n {found_words}\")\n",
    "\n",
    "# Text source: https://www.newsinlevels.com/products/worlds-dirtiest-city-level-3/\n",
    "\n",
    "example=\"\"\"Delhi’s air quality index (AQI) reached a severe level of 418, far above the ideal range of 0-50. \n",
    "Every winter, Delhi struggles with smog caused by various factors, including cold temperatures, crop burning, dust, vehicle emissions, and low wind speeds. \n",
    "Residents experience burning eyes, itchy throats, and difficulty breathing.\n",
    "While some can use air purifiers indoors, many must work outside, facing hazardous conditions. \n",
    "Pollution severely impacts vulnerable groups like children and the elderly, leading to health problems such as respiratory issues. \n",
    "Experts criticize the lack of long-term solutions, noting that government measures are often reactive.\"\"\"\n",
    "\n",
    "return_part_of_speech_from_text(example,\"noun\")\n",
    "\n",
    "return_part_of_speech_from_text(example,\"verb\")\n",
    "\n",
    "return_part_of_speech_from_text(example,\"adjective\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zadanie 3\n",
    "\n",
    "Korzystając z toolkitu SpaCy oraz modelu en_core_web_sm napisz program dokonujący analizy dokumentu tekstowego w języku angielskim o wydarzeniach ekonomicznych w języku angielskim. Zadaniem programu jest ekstrakcja następujących jednostek nazwanych (korzystając z modułu NER - Named Entity Recognition):\n",
    "* ORGANIZATION (e.g., company names)\n",
    "* DATE (e.g., contract start/end dates)\n",
    "* MONEY (e.g., payment amounts)\n",
    "\n",
    "Program musi wypisać znalezione jednostki i ich wartości."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities found in text: \n",
      "Organizations:\n",
      "['Google', 'Microsoft']\n",
      "Dates:\n",
      "['January 15, 2023', 'March 1, 2024', 'June 15, 2024']\n",
      "Money amounts: \n",
      "['100,000,000', '300,000,000']\n",
      "\n",
      "\n",
      "\n",
      "Entities found in text: \n",
      "Organizations:\n",
      "['Renault', 'IPO', 'Renault', 'IPO', 'IPO', 'Renault', 'EV', 'Twingo Legend']\n",
      "Dates:\n",
      "['2025', '2031', '2025', '2030', '2024']\n",
      "Money amounts: \n",
      "['10 billion €', '25 billion €', '20,000 €']\n"
     ]
    }
   ],
   "source": [
    "# Solution 3\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "The agreement between Google and Microsoft was signed on January 15, 2023.\n",
    "The payment of $100,000,000 is due on March 1, 2024. Another payment of $300,000,000 is scheduled for June 15, 2024.\n",
    "\"\"\"\n",
    "\n",
    "# Dodaj tez swoj tekst\n",
    "# My example, source: https://www.newsinlevels.com/products/renault-electric-cars-level-3/\n",
    "# Changed \"euros\" to \"€\".\n",
    "\n",
    "example=\"\"\"\n",
    "Renault aims to boost confidence in its electric vehicles unit, \n",
    "Ampere, projecting a revenue surge to 10 billion € in 2025 and 25 billion € in 2031.\n",
    "The company disclosed financial objectives, including break-even in 2025 and a 10% operating margin by 2030, \n",
    "ahead of a planned market listing in 2024. The IPO faces challenges due to sluggish electric vehicle demand, \n",
    "market uncertainties, and heightened Chinese competition. \n",
    "CEO Luca de Meo emphasized Renault’s cash capacity for Ampere’s growth but preferred the IPO for accelerated development. \n",
    "However, he asserted that if the valuation falls below an undisclosed threshold, the IPO plan would be abandoned.\n",
    "Renault also introduced a new EV model Twingo Legend priced below 20,000 €, \n",
    "aiming to compete with affordable Chinese alternatives.\n",
    "\"\"\"\n",
    "\n",
    "import spacy\n",
    "\n",
    "def economical_text_analysis(text):\n",
    "    sp=spacy.load(\"en_core_web_sm\")\n",
    "    doc=sp(text)\n",
    "    organizations=[]\n",
    "    dates=[]\n",
    "    money_amounts=[]\n",
    "    for ent in doc.ents:\n",
    "        match (ent.label_):\n",
    "            case \"ORG\":\n",
    "                organizations.append(ent.text)\n",
    "            case \"DATE\":\n",
    "                dates.append(ent.text)\n",
    "            case \"MONEY\":\n",
    "                money_amounts.append(ent.text)\n",
    "    print(\"Entities found in text: \")\n",
    "    print(\"Organizations:\")\n",
    "    print(organizations)\n",
    "    print(\"Dates:\")\n",
    "    print(dates)\n",
    "    print(\"Money amounts: \")\n",
    "    print(money_amounts)\n",
    "\n",
    "economical_text_analysis(text)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "economical_text_analysis(example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
